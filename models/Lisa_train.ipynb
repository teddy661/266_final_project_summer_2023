{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer, TFBertModel\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.get_logger().setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/tf/notebooks/w266/Final_Project/266_final_project_summer_2023/models')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_path = Path.cwd()\n",
    "script_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "if \"__file__\" in globals():\n",
    "    script_path = Path(__file__).parent.absolute()\n",
    "else:\n",
    "    script_path = Path.cwd()\n",
    "\n",
    "# setup for multi-gpu training\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "checkpoint_dir = script_path.joinpath(\"training_checkpoints\")\n",
    "checkpoint_fullpath = checkpoint_dir.joinpath(\"ckpt_{epoch:04d}.ckpt\")\n",
    "\n",
    "# load pkl file\n",
    "# print(\"Loading dev_examples.pkl\")\n",
    "# dev_example_path = script_path.joinpath(\"dev_examples.pkl\")\n",
    "# dev_examples = joblib.load(dev_example_path, pickle.HIGHEST_PROTOCOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading squadv2_train_tf\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from cache\n",
    "print(\"Loading squadv2_train_tf\")\n",
    "tf_dataset_path = script_path.joinpath(\n",
    "    \"squadv2_train_tf\"\n",
    ")\n",
    "ds_train = tf.data.Dataset.load(str(tf_dataset_path))\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "max_seq_length = 386\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_qa_model(\n",
    "    MODEL_NAME=\"bert-large-uncased\",\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n",
    "):\n",
    "    with mirrored_strategy.scope():\n",
    "        bert_config = BertConfig.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "        bert_model = TFBertModel.from_pretrained(MODEL_NAME, config=bert_config)\n",
    "\n",
    "        input_ids = tf.keras.layers.Input(\n",
    "            shape=(max_seq_length,), dtype=tf.int64, name=\"input_ids\"\n",
    "        )\n",
    "        attention_mask = tf.keras.layers.Input(\n",
    "            shape=(max_seq_length,), dtype=tf.int64, name=\"input_masks\"\n",
    "        )\n",
    "        token_type_ids = tf.keras.layers.Input(\n",
    "            shape=(max_seq_length,), dtype=tf.int64, name=\"token_type_ids\"\n",
    "        )\n",
    "\n",
    "        bert_inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "        sequence_embeddings = bert_model(bert_inputs).last_hidden_state\n",
    "\n",
    "        logits = tf.keras.layers.Dense(2, name=\"logits\")(sequence_embeddings)\n",
    "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
    "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
    "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
    "\n",
    "        softmax_start_logits = tf.keras.layers.Softmax()(start_logits)\n",
    "        softmax_end_logits = tf.keras.layers.Softmax()(end_logits)\n",
    "\n",
    "        # Need to do argmax after softmax to get most likely index\n",
    "        bert_qa_model = tf.keras.Model(\n",
    "            inputs=[input_ids, token_type_ids, attention_mask],\n",
    "            outputs=[softmax_start_logits, softmax_end_logits],\n",
    "        )\n",
    "\n",
    "        bert_qa_model.trainable = True\n",
    "\n",
    "        bert_qa_model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=[\n",
    "                tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            ],\n",
    "            metrics=[\n",
    "                tf.keras.metrics.SparseCategoricalAccuracy(name=\"start_accuracy\"),\n",
    "                tf.keras.metrics.SparseCategoricalAccuracy(name=\"end_accuracy\"),\n",
    "            ],\n",
    "        )\n",
    "    return bert_qa_model\n",
    "\n",
    "def return_prediction_string(bert_tokenizer, input_ids, predictions):\n",
    "    pass\n",
    "\n",
    "def combine_bert_subwords(bert_tokenizer, input_ids, predictions):\n",
    "    all_predictions = []\n",
    "    for x in range(len(predictions[0])):\n",
    "        answer = \"\"\n",
    "        token_list = bert_tokenizer.convert_ids_to_tokens(\n",
    "            input_ids[x][\n",
    "                np.argmax(predictions[0][x]) : np.argmax(predictions[1][x]) + 1\n",
    "            ]\n",
    "        )\n",
    "        if len(token_list) == 0:\n",
    "            answer = \"\"\n",
    "        elif token_list[0] == \"[CLS]\":\n",
    "            answer = \"\"\n",
    "        else:\n",
    "            for i, token in enumerate(token_list):\n",
    "                if token.startswith(\"##\"):\n",
    "                    answer += token[2:]\n",
    "                else:\n",
    "                    if i != 0:\n",
    "                        answer += \" \"\n",
    "                    answer += token\n",
    "        all_predictions.append(answer)\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 01:57:29.336826: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Prepare data...\")\n",
    "# sample dataset for predictions\n",
    "# samples = ds_train.take(ds_train.cardinality().numpy())\n",
    "samples = ds_train.take(1000)\n",
    "input_ids = []\n",
    "input_ids = []\n",
    "token_type_ids = []\n",
    "attention_mask = []\n",
    "impossible = []\n",
    "qas_id = []\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for sample in samples:\n",
    "    input_ids.append(sample[0][\"input_ids\"])\n",
    "    token_type_ids.append(sample[0][\"token_type_ids\"])\n",
    "    attention_mask.append(sample[0][\"attention_mask\"])\n",
    "    impossible.append(sample[1][\"is_impossible\"].numpy())\n",
    "    qas_id.append(sample[0][\"qas_id\"].numpy().decode(\"utf-8\"))\n",
    "    start_positions.append(sample[1][\"start_positions\"])\n",
    "    end_positions.append(sample[1][\"end_positions\"])\n",
    "\n",
    "input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int64)\n",
    "token_type_ids = tf.convert_to_tensor(token_type_ids, dtype=tf.int64)\n",
    "attention_mask = tf.convert_to_tensor(attention_mask, dtype=tf.int64)\n",
    "start_positions = tf.convert_to_tensor(start_positions, dtype=tf.int64)\n",
    "end_positions = tf.convert_to_tensor(end_positions, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(386,), dtype=int64, numpy=\n",
       "array([  101,  2043,  2106, 20773,  2707,  3352,  2759,  1029,   102,\n",
       "       20773, 21025, 19358, 22815,  1011,  5708,  1006,  1013, 12170,\n",
       "       23432, 29715,  3501, 29678, 12325, 29685,  1013, 10506,  1011,\n",
       "       10930,  2078,  1011,  2360,  1007,  1006,  2141,  2244,  1018,\n",
       "        1010,  3261,  1007,  2003,  2019,  2137,  3220,  1010,  6009,\n",
       "        1010,  2501,  3135,  1998,  3883,  1012,  2141,  1998,  2992,\n",
       "        1999,  5395,  1010,  3146,  1010,  2016,  2864,  1999,  2536,\n",
       "        4823,  1998,  5613,  6479,  2004,  1037,  2775,  1010,  1998,\n",
       "        3123,  2000,  4476,  1999,  1996,  2397,  4134,  2004,  2599,\n",
       "        3220,  1997,  1054,  1004,  1038,  2611,  1011,  2177, 10461,\n",
       "        1005,  1055,  2775,  1012,  3266,  2011,  2014,  2269,  1010,\n",
       "       25436, 22815,  1010,  1996,  2177,  2150,  2028,  1997,  1996,\n",
       "        2088,  1005,  1055,  2190,  1011,  4855,  2611,  2967,  1997,\n",
       "        2035,  2051,  1012,  2037, 14221,  2387,  1996,  2713,  1997,\n",
       "       20773,  1005,  1055,  2834,  2201,  1010, 20754,  1999,  2293,\n",
       "        1006,  2494,  1007,  1010,  2029,  2511,  2014,  2004,  1037,\n",
       "        3948,  3063,  4969,  1010,  3687,  2274,  8922,  2982,  1998,\n",
       "        2956,  1996,  4908,  2980,  2531,  2193,  1011,  2028,  3895,\n",
       "        1000,  4689,  1999,  2293,  1000,  1998,  1000,  3336,  2879,\n",
       "        1000,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change optimizer based on\n",
    "# https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert\n",
    "# https://arxiv.org/pdf/1810.04805.pdf\n",
    "epochs = 1\n",
    "batch_size = 48\n",
    "steps_per_epoch = len(input_ids) // batch_size\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = num_train_steps // 10\n",
    "initial_learning_rate = 5e-5\n",
    "\n",
    "optimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=initial_learning_rate)\n",
    "\n",
    "bert_qa_model = create_bert_qa_model(optimizer=optimizer)\n",
    "# tf.keras.utils.plot_model(bert_qa_model, show_shapes=True)\n",
    "# bert_qa_model.summary()\n",
    "\n",
    "history = bert_qa_model.fit(\n",
    "    [input_ids, token_type_ids, attention_mask],\n",
    "    [start_positions, end_positions],\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_fullpath,\n",
    "            verbose=1,\n",
    "            save_weights_only=True,\n",
    "            save_freq=\"epoch\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "joblib.dump(\n",
    "    history,\n",
    "    \"bert-model-train-history.pkl\",\n",
    "    compress=False,\n",
    "    protocol=pickle.HIGHEST_PROTOCOL,\n",
    ")\n",
    "\n",
    "# bert_qa_model.save_weights(\"backupsaveend.h5\")\n",
    "\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Execute predictions...\")\n",
    "new_predictions = bert_qa_model.predict([input_ids, token_type_ids, attention_mask])\n",
    "\n",
    "\n",
    "print(\"Done with Predictions...\")\n",
    "new_answers = combine_bert_subwords(bert_tokenizer, input_ids, new_predictions)\n",
    "\n",
    "print(\"Calculate probabilities for split answers...\")\n",
    "probabilities = []\n",
    "for i, prediction in enumerate(new_predictions[0]):\n",
    "    probabilities.append(\n",
    "        np.amax(new_predictions[0][i]) * np.amax(new_predictions[1][i])\n",
    "    )\n",
    "\n",
    "print(\"Choose best answer for split answers...\")\n",
    "\n",
    "\n",
    "# duplicate_ids = [ x for x,  count in collections.Counter(qas_id).items() if count > 1]\n",
    "def list_duplicates(seq):\n",
    "    tally = defaultdict(list)\n",
    "    for i, item in enumerate(seq):\n",
    "        tally[item].append(i)\n",
    "    return ((key, locs) for key, locs in tally.items() if len(locs) > 1)\n",
    "\n",
    "\n",
    "duplicate_ids = sorted(list_duplicates(qas_id))\n",
    "\n",
    "scoring_dict = {}\n",
    "for d in duplicate_ids:\n",
    "    maxp = None\n",
    "    for i in d[1]:\n",
    "        if maxp == None or probabilities[i] > maxp:\n",
    "            maxp = probabilities[i]\n",
    "            maxindex = i\n",
    "    scoring_dict[qas_id[maxindex]] = new_answers[maxindex]\n",
    "    print(f\"{scoring_dict[qas_id[maxindex]]} {maxp}\")\n",
    "for i, q in enumerate(new_answers):\n",
    "    if qas_id[i] not in scoring_dict:\n",
    "        scoring_dict[qas_id[i]] = q\n",
    "\n",
    "# diagnose impossible questions Highly inefficient\n",
    "for i, q in enumerate(qas_id):\n",
    "    answer = \"\"\n",
    "    question = \"\"\n",
    "    for t in train_examples:\n",
    "        if t.qas_id == qas_id[i]:\n",
    "            answer = t.answer_text\n",
    "            question = t.question_text\n",
    "            break\n",
    "    if impossible[i] == 1:\n",
    "        print(f\"Index: {i}\")\n",
    "        print(f\"QAS_ID: {qas_id[i]}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(f\"Prediction: {new_answers[i]}\")\n",
    "        print(80 * \"-\")\n",
    "\n",
    "with open(\"scoring_dict.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(scoring_dict, f, ensure_ascii=False, indent=4)\n",
    "print(\"Wrote scoring_dict.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-15 20:20:11.966391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertConfig, BertTokenizer, TFBertModel, WarmUp\n",
    "\n",
    "from bert_large_uncased import create_bert_qa_model\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(\"INFO\")\n",
    "\n",
    "\n",
    "if \"__file__\" in globals():\n",
    "    script_path = Path(__file__).parent.absolute()\n",
    "else:\n",
    "    script_path = Path.cwd()\n",
    "\n",
    "# setup for multi-gpu training\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "checkpoint_dir = script_path.joinpath(\"training_checkpoints\")\n",
    "checkpoint_fullpath = checkpoint_dir.joinpath(\"ckpt_{epoch:04d}.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/notebooks/w266/Final_Project/266_final_project_summer_2023/models'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading squadv2_dev_tf\n"
     ]
    }
   ],
   "source": [
    "# load pkl file\n",
    "# print(\"Loading dev_examples.pkl\")\n",
    "# dev_example_path = script_path.joinpath(\"dev_examples.pkl\")\n",
    "# dev_examples = joblib.load(dev_example_path, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load dataset from cache\n",
    "print(\"Loading squadv2_dev_tf\")\n",
    "tf_dataset_path = script_path.joinpath(\"squadv2_train_tf\")\n",
    "# tf_dataset_path = script_path.joinpath(\"../cache/squadv2_train_tf\")\n",
    "ds_train = tf.data.Dataset.load(str(tf_dataset_path))\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m start_positions \u001b[39m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m end_positions \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 41\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m samples:\n\u001b[1;32m     42\u001b[0m     input_ids\u001b[39m.\u001b[39mappend(sample[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     43\u001b[0m     token_type_ids\u001b[39m.\u001b[39mappend(sample[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/python/py311/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:797\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    796\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 797\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[1;32m    798\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    799\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/python/py311/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:780\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[0;32m--> 780\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[1;32m    781\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[1;32m    782\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[1;32m    783\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[1;32m    785\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/python/py311/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3011\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3009\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3010\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3011\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3012\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mIteratorGetNext\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, iterator, \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types,\n\u001b[1;32m   3013\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes)\n\u001b[1;32m   3014\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3015\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def return_prediction_string(bert_tokenizer, input_ids, predictions):\n",
    "    pass\n",
    "\n",
    "\n",
    "def combine_bert_subwords(bert_tokenizer, input_ids, predictions):\n",
    "    all_predictions = []\n",
    "    for x in range(len(predictions[0])):\n",
    "        answer = \"\"\n",
    "        token_list = bert_tokenizer.convert_ids_to_tokens(\n",
    "            input_ids[x][np.argmax(predictions[0][x]) : np.argmax(predictions[1][x]) + 1]\n",
    "        )\n",
    "        if len(token_list) == 0:\n",
    "            answer = \"\"\n",
    "        elif token_list[0] == \"[CLS]\":\n",
    "            answer = \"\"\n",
    "        else:\n",
    "            for i, token in enumerate(token_list):\n",
    "                if token.startswith(\"##\"):\n",
    "                    answer += token[2:]\n",
    "                else:\n",
    "                    if i != 0:\n",
    "                        answer += \" \"\n",
    "                    answer += token\n",
    "        all_predictions.append(answer)\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "print(\"Prepare data...\")\n",
    "# sample dataset for predictions\n",
    "samples = ds_train.take(ds_train.cardinality().numpy())\n",
    "# samples = ds_train.take(1000)\n",
    "input_ids = []\n",
    "input_ids = []\n",
    "token_type_ids = []\n",
    "attention_mask = []\n",
    "impossible = []\n",
    "qas_id = []\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for sample in samples:\n",
    "    input_ids.append(sample[0][\"input_ids\"])\n",
    "    token_type_ids.append(sample[0][\"token_type_ids\"])\n",
    "    attention_mask.append(sample[0][\"attention_mask\"])\n",
    "    impossible.append(sample[1][\"is_impossible\"].numpy())\n",
    "    qas_id.append(sample[0][\"qas_id\"].numpy().decode(\"utf-8\"))\n",
    "    start_positions.append(sample[1][\"start_positions\"])\n",
    "    end_positions.append(sample[1][\"end_positions\"])\n",
    "\n",
    "input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int64)\n",
    "token_type_ids = tf.convert_to_tensor(token_type_ids, dtype=tf.int64)\n",
    "attention_mask = tf.convert_to_tensor(attention_mask, dtype=tf.int64)\n",
    "start_positions = tf.convert_to_tensor(start_positions, dtype=tf.int64)\n",
    "end_positions = tf.convert_to_tensor(end_positions, dtype=tf.int64)\n",
    "\n",
    "# Change optimizer based on\n",
    "# https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert\n",
    "# https://arxiv.org/pdf/1810.04805.pdf\n",
    "epochs = 6\n",
    "batch_size = 48\n",
    "steps_per_epoch = len(input_ids) // batch_size\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = num_train_steps // 10\n",
    "initial_learning_rate = 2e-5\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        end_learning_rate=0,\n",
    "        decay_steps=num_train_steps,\n",
    "    )\n",
    "\n",
    "    warmup_schedule = WarmUp(\n",
    "        initial_learning_rate=0,\n",
    "        decay_schedule_fn=linear_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "    )\n",
    "\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=warmup_schedule)\n",
    "\n",
    "    bert_qa_model = create_bert_qa_model(optimizer=optimizer)\n",
    "    bert_qa_model.load_weights(\"./earlier-training/training_checkpoints/ckpt_0004.ckpt\")\n",
    "    bert_qa_model.trainable = True\n",
    "    bert_qa_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=[\n",
    "            tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        ],\n",
    "        metrics=[\n",
    "            tf.keras.metrics.SparseCategoricalAccuracy(name=\"start_accuracy\"),\n",
    "            tf.keras.metrics.SparseCategoricalAccuracy(name=\"end_accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "history = bert_qa_model.fit(\n",
    "    [input_ids, token_type_ids, attention_mask],\n",
    "    [start_positions, end_positions],\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_fullpath,\n",
    "            verbose=1,\n",
    "            save_weights_only=True,\n",
    "            save_freq=\"epoch\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Save history...\")\n",
    "joblib.dump(\n",
    "    history.history,\n",
    "    \"bert-model-train-history.pkl\",\n",
    "    compress=False,\n",
    "    protocol=pickle.HIGHEST_PROTOCOL,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

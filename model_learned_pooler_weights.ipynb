{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(\"INFO\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start generator with training labels, pointing to data directory with embeddings\n",
    "\n",
    "\n",
    "import models.bert_learned_pooler as bert_learned_pooler\n",
    "\n",
    "bert_model = bert_learned_pooler.create_learned_pooler(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f8dfe86ed10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.load_weights(\"./models/learned_pooler_epochs_1/training_checkpoints/ckpt_0001.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"learned_pooler_epochs_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_masks (InputLayer)       [(None, 386)]        0           []                               \n",
      "                                                                                                  \n",
      " input_ids (InputLayer)         [(None, 386)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer)    [(None, 386)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  335141888   ['input_masks[0][0]',            \n",
      "                                thPoolingAndCrossAt               'input_ids[0][0]',              \n",
      "                                tentions(last_hidde               'token_type_ids[0][0]']         \n",
      "                                n_state=(None, 386,                                               \n",
      "                                 1024),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 1024),                                                         \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=(                                               \n",
      "                                (None, 386, 1024),                                                \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024),                                               \n",
      "                                 (None, 386, 1024))                                               \n",
      "                                , attentions=None,                                                \n",
      "                                cross_attentions=No                                               \n",
      "                                ne)                                                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose (TFOpLa  (None, 386, 1024, 2  0          ['tf_bert_model[1][0]',          \n",
      " mbda)                          5)                                'tf_bert_model[1][1]',          \n",
      "                                                                  'tf_bert_model[1][2]',          \n",
      "                                                                  'tf_bert_model[1][3]',          \n",
      "                                                                  'tf_bert_model[1][4]',          \n",
      "                                                                  'tf_bert_model[1][5]',          \n",
      "                                                                  'tf_bert_model[1][6]',          \n",
      "                                                                  'tf_bert_model[1][7]',          \n",
      "                                                                  'tf_bert_model[1][8]',          \n",
      "                                                                  'tf_bert_model[1][9]',          \n",
      "                                                                  'tf_bert_model[1][10]',         \n",
      "                                                                  'tf_bert_model[1][11]',         \n",
      "                                                                  'tf_bert_model[1][12]',         \n",
      "                                                                  'tf_bert_model[1][13]',         \n",
      "                                                                  'tf_bert_model[1][14]',         \n",
      "                                                                  'tf_bert_model[1][15]',         \n",
      "                                                                  'tf_bert_model[1][16]',         \n",
      "                                                                  'tf_bert_model[1][17]',         \n",
      "                                                                  'tf_bert_model[1][18]',         \n",
      "                                                                  'tf_bert_model[1][19]',         \n",
      "                                                                  'tf_bert_model[1][20]',         \n",
      "                                                                  'tf_bert_model[1][21]',         \n",
      "                                                                  'tf_bert_model[1][22]',         \n",
      "                                                                  'tf_bert_model[1][23]',         \n",
      "                                                                  'tf_bert_model[1][24]']         \n",
      "                                                                                                  \n",
      " learned_pooler (LearnedPooler)  (None, 386, 1024)   26          ['tf.compat.v1.transpose[0][0]'] \n",
      "                                                                                                  \n",
      " logits (Dense)                 (None, 386, 2)       2050        ['learned_pooler[0][0]']         \n",
      "                                                                                                  \n",
      " tf.split_1 (TFOpLambda)        [(None, 386, 1),     0           ['logits[0][0]']                 \n",
      "                                 (None, 386, 1)]                                                  \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_2 (TFOpLa  (None, 386)         0           ['tf.split_1[0][0]']             \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_3 (TFOpLa  (None, 386)         0           ['tf.split_1[0][1]']             \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " softmax_2 (Softmax)            (None, 386)          0           ['tf.compat.v1.squeeze_2[0][0]'] \n",
      "                                                                                                  \n",
      " softmax_3 (Softmax)            (None, 386)          0           ['tf.compat.v1.squeeze_3[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 335,143,964\n",
      "Trainable params: 2,076\n",
      "Non-trainable params: 335,141,888\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the weights for all 25 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -0.7286164,\n",
       " 1: -0.68705434,\n",
       " 2: -0.7754198,\n",
       " 3: -0.7786671,\n",
       " 4: -0.6581254,\n",
       " 5: -0.84351796,\n",
       " 6: -0.7038068,\n",
       " 7: -0.78308666,\n",
       " 8: -0.69364554,\n",
       " 9: -0.7104143,\n",
       " 10: -0.74511576,\n",
       " 11: -0.67180556,\n",
       " 12: -0.5480674,\n",
       " 13: -0.5075096,\n",
       " 14: -0.6255021,\n",
       " 15: -0.65797615,\n",
       " 16: -0.6923896,\n",
       " 17: -0.8698429,\n",
       " 18: -0.75600356,\n",
       " 19: -0.25995982,\n",
       " 20: 0.2591863,\n",
       " 21: 0.38232687,\n",
       " 22: 0.6862729,\n",
       " 23: 0.9188513,\n",
       " 24: 1.3424544}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = bert_model.weights[-4].numpy()\n",
    "t = bert_model.weights[-3].numpy()\n",
    "layer_to_weight = dict(zip(range(len(w)), w))\n",
    "layer_to_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top 12 absolute weights from layer_to_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 23, 17,  5,  7,  3,  2, 18, 10,  0,  9,  6])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_12_absolute = abs(w).argsort()[-12:][::-1]\n",
    "top_12_absolute\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models are compiled as a bi-headed model, the first representing span start position and the second representing span end position. No activation is applied as the heads come directly from splitting a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # monitor accuracy during the training process\n",
    "    model.compile(loss=[loss, loss], optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Gather a list of models to fit\n",
    "# Since fitting is typically faster than data load, it is beneficial to many models at once\n",
    "model_list = [average_pooler_model, learned_pooling_model]\n",
    "\n",
    "for model_current in model_list:\n",
    "    compile_model(model_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import models.bert_embedding_parser as bert_embedding_parser\n",
    "\n",
    "gen = bert_embedding_parser.load_bert_embeddings(bert_model, batch_size=4)\n",
    "\n",
    "i = 0\n",
    "max_batches = 8248  # Can be any number; this is pre-calculated based on the amount of training data used; 8248 goes through entire dataset at batch size of 16\n",
    "\n",
    "\n",
    "for batch in gen:\n",
    "    # Read in the batch of data from generator\n",
    "    X = batch[0]\n",
    "    Y = batch[1]\n",
    "\n",
    "    for model_current in model_list:\n",
    "        # Fit the generated dataset once\n",
    "        model_current.fit(X, Y, epochs=1)\n",
    "\n",
    "    # increment counter\n",
    "    i += 1\n",
    "    del batch  # delete the batch to free up memory\n",
    "\n",
    "    # When the number of\n",
    "    if i == max_batches:  # 4 batches; can save each quarter\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "weights_dir = \"weights\"\n",
    "n = 0\n",
    "for m in model_list:\n",
    "    n += 1\n",
    "    m.save_weights(weights_dir + \"/%s.h5\" % m.name)\n",
    "\n",
    "\n",
    "# TODO: print out the 25 weights for each model (before and after fine-tuning)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

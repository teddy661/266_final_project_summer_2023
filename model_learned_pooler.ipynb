{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(\"INFO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the appropriate datasets and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131911, 2)\n"
     ]
    }
   ],
   "source": [
    "import models.data_load as data_load\n",
    "\n",
    "(\n",
    "    train_input_ids,\n",
    "    train_token_type_ids,\n",
    "    train_mask,\n",
    "    train_impossible,\n",
    "    train_start_positions,\n",
    "    train_end_positions,\n",
    "    qas_id,\n",
    ") = data_load.load_train()\n",
    "\n",
    "train_labels = np.vstack([train_start_positions, train_end_positions]).T\n",
    "print(train_labels.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned and average pooling alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"learned_pooler\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 386, 1024,   0           []                               \n",
      "                                25)]                                                              \n",
      "                                                                                                  \n",
      " learned_pooler (LearnedPooler)  (None, 386, 1024, 1  26         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 386, 1024, 2  4           ['learned_pooler[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.split (TFOpLambda)          [(None, 386, 1024,   0           ['dense[0][0]']                  \n",
      "                                1),                                                               \n",
      "                                 (None, 386, 1024,                                                \n",
      "                                1)]                                                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze (TFOpLamb  (None, 386, 1024)   0           ['tf.split[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_1 (TFOpLa  (None, 386, 1024)   0           ['tf.split[0][1]']               \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from layers.learned_pooler import get_learned_pooling_model\n",
    "\n",
    "learned_pooling_model = get_learned_pooling_model()\n",
    "learned_pooling_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"average_pooler\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 386, 1024,   0           []                               \n",
      "                                25)]                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 386, 1024, 1  0          ['input_2[0][0]']                \n",
      " a)                             )                                                                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 386, 1024, 2  4           ['tf.math.reduce_mean[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.split_1 (TFOpLambda)        [(None, 386, 1024,   0           ['dense_1[0][0]']                \n",
      "                                1),                                                               \n",
      "                                 (None, 386, 1024,                                                \n",
      "                                1)]                                                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_2 (TFOpLa  (None, 386, 1024)   0           ['tf.split_1[0][0]']             \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_3 (TFOpLa  (None, 386, 1024)   0           ['tf.split_1[0][1]']             \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from layers.average_pooler import get_average_pooler\n",
    "\n",
    "average_pooler_model = get_average_pooler()\n",
    "average_pooler_model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f2882ca91d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start generator with training labels, pointing to data directory with embeddings\n",
    "\n",
    "\n",
    "import models.bert_large_uncased as bert_large_uncased\n",
    "\n",
    "bert_model = bert_large_uncased.create_bert_qa_model(optimizer=\"adam\")\n",
    "bert_model.load_weights(\"./results/bert-large-uncased/training_checkpoints/ckpt_0004.ckpt\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models are compiled as a bi-headed model, the first representing span start position and the second representing span end position. No activation is applied as the heads come directly from splitting a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # monitor accuracy during the training process\n",
    "    model.compile(loss=[loss, loss], optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Gather a list of models to fit\n",
    "# Since fitting is typically faster than data load, it is beneficial to many models at once\n",
    "model_list = [average_pooler_model, learned_pooling_model]\n",
    "\n",
    "for model_current in model_list:\n",
    "    compile_model(model_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 11s 11s/step\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'Equal' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/python/py311/lib/python3.11/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/opt/python/py311/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/python/py311/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"/opt/python/py311/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"/opt/python/py311/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/python/py311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/opt/python/py311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/opt/python/py311/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/python/py311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/python/py311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/python/py311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_21214/3157361031.py\", line 14, in <module>\n      learned_pooling_model.fit(X, Y, epochs=1)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/training.py\", line 1055, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/training.py\", line 1149, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/metrics/base_metric.py\", line 691, in update_state\n      matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/metrics/accuracy_metrics.py\", line 459, in sparse_categorical_accuracy\n      matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/utils/metrics_utils.py\", line 971, in sparse_categorical_matches\n      matches = tf.cast(tf.equal(y_true, y_pred), backend.floatx())\nNode: 'Equal'\nrequired broadcastable shapes\n\t [[{{node Equal}}]] [Op:__inference_train_function_36436]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m X \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m Y \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m learned_pooling_model\u001b[39m.\u001b[39;49mfit(X, Y, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m \u001b[39m# increment counter\u001b[39;00m\n\u001b[1;32m     17\u001b[0m i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/python/py311/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/python/py311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'Equal' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/python/py311/lib/python3.11/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/opt/python/py311/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/python/py311/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"/opt/python/py311/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"/opt/python/py311/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/opt/python/py311/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/python/py311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/opt/python/py311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/opt/python/py311/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/python/py311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/python/py311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/python/py311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_21214/3157361031.py\", line 14, in <module>\n      learned_pooling_model.fit(X, Y, epochs=1)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/training.py\", line 1055, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/training.py\", line 1149, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/metrics/base_metric.py\", line 691, in update_state\n      matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/metrics/accuracy_metrics.py\", line 459, in sparse_categorical_accuracy\n      matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/opt/python/py311/lib/python3.11/site-packages/keras/utils/metrics_utils.py\", line 971, in sparse_categorical_matches\n      matches = tf.cast(tf.equal(y_true, y_pred), backend.floatx())\nNode: 'Equal'\nrequired broadcastable shapes\n\t [[{{node Equal}}]] [Op:__inference_train_function_36436]"
     ]
    }
   ],
   "source": [
    "import bert_embedding_parser as bert_embedding_parser\n",
    "\n",
    "gen = bert_embedding_parser.load_bert_embeddings(bert_model, batch_size=16)\n",
    "\n",
    "i = 0\n",
    "max_batches = 8248  # Can be any number; this is pre-calculated based on the amount of training data used; 8248 goes through entire dataset at batch size of 16\n",
    "\n",
    "\n",
    "for batch in gen:\n",
    "    # Read in the batch of data from generator\n",
    "    X = batch[0]\n",
    "    Y = batch[1]\n",
    "\n",
    "    learned_pooling_model.fit(X, Y, epochs=1)\n",
    "\n",
    "    # increment counter\n",
    "    i += 1\n",
    "\n",
    "    # When the number of\n",
    "    if i == max_batches:  # 4 batches; can save each quarter\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import data.bert_embedding_parser as bert_embedding_parser\n",
    "\n",
    "gen = bert_embedding_parser.load_bert_embeddings(bert_model, batch_size=16)\n",
    "\n",
    "i = 0\n",
    "max_batches = 8248  # Can be any number; this is pre-calculated based on the amount of training data used; 8248 goes through entire dataset at batch size of 16\n",
    "\n",
    "\n",
    "for batch in gen:\n",
    "    # Read in the batch of data from generator\n",
    "    X = batch[0]\n",
    "    Y = batch[1]\n",
    "\n",
    "    for model_current in model_list:\n",
    "        # Fit the generated dataset once\n",
    "        model_current.fit(X, Y, epochs=1)\n",
    "\n",
    "    # increment counter\n",
    "    i += 1\n",
    "\n",
    "    # When the number of\n",
    "    if i == max_batches:  # 4 batches; can save each quarter\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "weights_dir = \"weights\"\n",
    "n = 0\n",
    "for m in model_list:\n",
    "    n += 1\n",
    "    m.save_weights(weights_dir + \"/%s.h5\" % m.name)\n",
    "\n",
    "\n",
    "# TODO: print out the 25 weights for each model (before and after fine-tuning)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
